1c1
< import argparse, sys, os, time, json
---
> import argparse, sys, os, time
7,10c7,10
< from .model import *
< from .utils import compute_parallel, merge_nested_dict, non_even_split
< from . import dataprocessing
< from .decoding import BeamSearchKeys, length_penalty
---
> from .lm import DecoderLanguageModel, DecoderLanguageModel_V2
> from ..components.utils import *
> from ..components import dataprocessing
> from .datasetloader import make_const_capacity_batch_list
13c13
<     def __init__(self, model_dir, model=None, graph=None, checkpoint=None, n_gpus=1, n_cpu_cores=4, batch_capacity=None, decode_config=None):
---
>     def __init__(self, model_dir, model=None, graph=None, checkpoint=None, n_gpus=1, n_cpu_cores=4, batch_capacity=None):
23c23
<         with open(self.model_dir + '/model_config.py', 'r') as f:
---
>         with open(self.model_dir + '/lm_config.py', 'r') as f:
29,34d28
<         # Merging the default decoding setting and the one specified at runtime.
<         if not 'decode_config' in params['test']:
<             params['test']['decode_config'] = {}
<         merge_nested_dict(params['test']['decode_config'], decode_config)
<         logger.debug('Decode configuration: ' + str(params['test']['decode_config']))
< 
37c31
<             params["vocab"]["target_dict"],
---
>             params["vocab"]["dict"],
66c60
<                 self.model = Transformer(params)
---
>                 self.model = DecoderLanguageModel(params)
84,86c78
<                 'x_len': tf.placeholder(tf.int32, [None]),
<                 'init_y': tf.placeholder(tf.int32, [None, None]),
<                 'init_y_len': tf.placeholder(tf.int32, [None])
---
>                 'x_len': tf.placeholder(tf.int32, [None])
91,92c83
<                 ((self.ph_dict['x'], self.ph_dict['x_len']),
<                 (self.ph_dict['init_y'], self.ph_dict['init_y_len'])), self.n_gpus)
---
>                 (self.ph_dict['x'], self.ph_dict['x_len']), self.n_gpus)
94,99d84
<             # computation graph for beam search
<             self.ph_beam_size = tf.placeholder(tf.int32, [])
<             self.op_beam_hypos_scores = self.make_op(
<                 self.fn_beam_search,
<                 self.ph_beam_size)
< 
103,116d87
<             # Computation graph for translation score
<             self.ph_length_penalty = tf.placeholder(tf.float64, [])
<             self.op_trans_score = self.make_op(self.fn_translation_score)
< 
<     def fn_beam_search(self, inputs, ph_beam_size):
<         (x, x_len), (init_y, init_y_len) = inputs
<         beam_candidates, scores = self.model.decode_V2(
<             x,
<             x_len,
<             ph_beam_size,
<             return_search_results=True,
<             init_y=init_y,
<             init_y_len=init_y_len)
<         return beam_candidates, scores
119,121c90,92
<         (x, x_len), (y, y_len) = inputs
<         logits = self.model.get_logits(x, y, x_len, y_len, False)
<         is_target = tf.sequence_mask(y_len, tf.shape(y)[1], dtype=tf.float32)
---
>         (x, x_len) = inputs
>         logits = self.model.get_logits(x, training=False)
>         is_target = tf.sequence_mask(x_len, tf.shape(x)[1], dtype=tf.float32)
124c95
<         log_prob = tf.batch_gather(log_prob_dist, y[:, :, None]) # [batch, length, 1]
---
>         log_prob = tf.batch_gather(log_prob_dist, x[:, :, None]) # [batch, length, 1]
127c98
<         perp = tf.exp(-seq_log_prob / tf.cast(y_len, tf.float32))
---
>         perp = tf.exp(-seq_log_prob / tf.cast(x_len, tf.float32))
132,149c103
<     def fn_translation_score(self, inputs):
<         (x, x_len), (y, y_len) = inputs
<         logits = self.model.get_logits(x, y, x_len, y_len, False)
<         is_target = tf.sequence_mask(y_len, tf.shape(y)[1], dtype=tf.float32)
< 
<         log_prob_dist = tf.math.log_softmax(logits, axis=-1) # [batch, length, vocab]
<         log_prob = tf.batch_gather(log_prob_dist, y[:, :, None]) # [batch, length, 1]
<         log_prob = log_prob[:, :, 0]
<         seq_log_prob = tf.reduce_sum(log_prob * is_target, axis=1) #[batch]
< 
<         # penalty coefficient (defined in model.py)
<         penalty = length_penalty(y_len, self.ph_length_penalty)
< 
<         score = seq_log_prob / penalty 
<         
<         return [score]
< 
<     def make_op(self, fn, *args, **kwargs):
---
>     def make_op(self, fn):
158c112
<         return compute_parallel(fn, self.inputs_parallel, *args, **kwargs)
---
>         return compute_parallel(fn, self.inputs_parallel)
161,164c115,116
<         return {self.ph_dict['x']: batch[0][0],
<                     self.ph_dict['x_len']: batch[0][1],
<                     self.ph_dict['init_y']: batch[1][0],
<                     self.ph_dict['init_y_len']: batch[1][1]}
---
>         return {self.ph_dict['x']: batch[0],
>                     self.ph_dict['x_len']: batch[1]}
217,226c169,170
<     def make_batches(self, x, y, batch_capacity=None):
<         batch_capacity = batch_capacity or (self.batch_capacity * 5)
<         return dataprocessing.make_batches_source_target_const_capacity_batch_from_list(
<             x, y,
<             self.params["vocab"]["source_dict"], self.params["vocab"]["target_dict"],
<             batch_capacity,
<             self.params["vocab"]["UNK_ID"],
<             self.params["vocab"]["PAD_ID"],
<             EOS_ID=self.params["vocab"]["EOS_ID"],
<             allow_skip=False)
---
>     def make_batches(self, x, batch_capacity=None):
>         batch_capacity = batch_capacity or self.batch_capacity
228,229c172,178
<     def calculate_sentence_perplexity(self, sources, targets):
<         batches = self.make_batches(sources, targets)
---
>         data = self.vocab.text2IDs(x, False)
>         batches = make_const_capacity_batch_list(data, [len(d) for d in data], batch_capacity, self.vocab.PAD_ID)
>         return batches
> 
> 
>     def calculate_sentence_perplexity(self, x):
>         batches = self.make_batches(x)
234,235c183,184
<     def calculate_corpus_perplexity(self, sources, targets):
<         batches = self.make_batches(sources, targets)
---
>     def calculate_corpus_perplexity(self, x):
>         batches = self.make_batches(x)
237c186
<         sent_lens = np.array(sum((batch[1][1] for batch in batches), []))
---
>         sent_lens = np.array(sum((batch[1] for batch in batches), []))
243,269c192,198
<     def calculate_translation_score(self, sources, targets, length_penalty_a=None):
<         if length_penalty_a is None:
<             length_penalty_a = self.params["test"]["decode_config"]["length_penalty_a"]
<         batches = self.make_batches(sources, targets)
<         scores, = self.execute_op(self.op_trans_score, batches, {self.ph_length_penalty: length_penalty_a})
<         return scores
< 
< 
<     def translate_sentences(self, texts, beam_size=1, return_search_results=False, init_y_texts=None):
< 
<         # Translate
<         batch_capacity = 5 * self.batch_capacity // beam_size
<         if init_y_texts is None:
<             init_y_texts = [''] * len(texts)
< 
<         batches = self.make_batches(texts, init_y_texts, batch_capacity)
<         candidates, scores = self.execute_op(self.op_beam_hypos_scores, batches, {self.ph_beam_size: beam_size})
< 
< 
<         if return_search_results:
<             nsamples = len(candidates)
<             # flatten 
<             candidates = sum(candidates, []) # [nsamples*beam_size, length(variable)]
<             # convert to string
<             candidates = self.IDs2text(candidates) #[nsamples*beam_size]
<             # restore shape
<             candidates = [candidates[i:i + beam_size] for i in range(0, len(candidates), beam_size)]
---
>     def calculate_log_prob(self, x):
>         batches = self.make_batches(x)
>         sent_perp, = self.execute_op(self.op_perplexity, batches)
>         sent_lens = np.array(sum((batch[1] for batch in batches), []))
>         logprob = - np.log(sent_perp) * sent_lens
>         return logprob
> 
271,276c200,205
<             return candidates, scores
<         else:
<             # take top 1
<             candidates = [beam[0] for beam in candidates] # [nsamples, length(variable)]
<             # convert to string
<             candidates = self.IDs2text(candidates) #[nsamples]
---
>     def calculate_pmi(self, context, x):
>         """PMI(context, x) = log p(context^x) - log p(context) - log p(x)"""
>         assert len(context) == len(x)
>         n = len(context)
>         probs = self.calculate_log_prob([c + ' ' + _x for c, _x in zip(context, x)] + context + x)
>         return probs[:n] - probs[n:n * 2] - probs[n*2:]
278d206
<             return candidates
280a209,211
>     # logger
>     basicConfig(level=INFO)
> 
282,287c213,217
<     TRANSLATE = 'translate'
<     PERPLEXITY = 'perplexity'
<     CORPUS_PERP = 'corpus_perplexity'
<     TRANS_DETAIL = 'translate_detail'
<     TRANS_SCORE = 'trans_score'
<     modes = [TRANSLATE, PERPLEXITY, CORPUS_PERP, TRANS_DETAIL, TRANS_SCORE]
---
>     PERPLEXITY = 'ppl'
>     CORPUS_PERP = 'corpus_ppl'
>     LOG_PROB = 'log_prob'
>     PMI = 'pmi'
>     modes = [PERPLEXITY, CORPUS_PERP, LOG_PROB, PMI]
292,293c222
<     parser.add_argument('optional', type=str, nargs='*')
<     parser.add_argument('--mode', type=str, choices=modes, default=TRANSLATE)
---
>     parser.add_argument('--mode', type=str, choices=modes, default=PERPLEXITY)
296d224
<     parser.add_argument('--decode-config-json', '--config', '-c', type=str, default=None)
299,306d226
<     parser.add_argument('--context_delimiter', type=str, default=None)
<     parser.add_argument('--beam_size', type=int, default=1)
<     parser.add_argument('--online', action='store_true')
<     parser.add_argument('--log-level', choices=["INFO", "DEBUG"], default="INFO")
<     args = parser.parse_args()
< 
<     # logger
<     basicConfig(level=(INFO if args.log_level == 'INFO' else 'DEBUG'))
308c228,231
<     dec_conf = json.loads(args.decode_config_json) if args.decode_config_json else None
---
>     # for PMI
>     parser.add_argument('--context', '-c', type=str, default=None)
>     parser.add_argument('--sentence', '-x', type=str, default=None)
>     args = parser.parse_args()
315,316c238
<         batch_capacity = args.batch_capacity,
<         decode_config = dec_conf)
---
>         batch_capacity = args.batch_capacity)
320,358c242,243
<     if args.mode == TRANSLATE or args.mode == TRANS_DETAIL:
<         def __trans(x ,y):
<             if args.mode == TRANSLATE:
<                 for line in inference.translate_sentences(x, args.beam_size, init_y_texts=y):
<                     print(line)
<             else:
<                 hyp, score = inference.translate_sentences(x, args.beam_size, True, init_y_texts=y)
<                 for _hyp, _score in zip(hyp, score):
<                     for sent,sent_score in zip(_hyp, _score): 
<                         print('{}\t{}'.format(sent_score, sent))
<             sys.stdout.flush()
<         
<         if args.online:
<             while True:
<                 try:
<                     line = sys.stdin.readline()
<                     if len(line) == 0:
<                         exit(0)
<                     if args.context_delimiter is not None:
<                         x, y = line.split(args.context_delimiter)
<                         x, y = [x], [y]
<                     else:
<                         x, y = [line], None
<                     __trans(x, y)
<                 except Exception as e:
<                     sys.stderr.write(e)
<         else:
<             if args.context_delimiter is not None:
<                 x, y = zip(*(line.split(args.context_delimiter) for line in sys.stdin))
<             else:
<                 x, y = [line.strip() for line in sys.stdin], None
<             __trans(x, y)
<          
<     elif args.mode == PERPLEXITY or args.mode == CORPUS_PERP or args.mode == TRANS_SCORE:
<         src_f, trg_f = args.optional[0], args.optional[1]
<         with open(src_f, 'r') as f:
<             x = [line.strip() for line in f]
<         with open(trg_f, 'r') as f:
<             y = [line.strip() for line in f]
---
>     if args.mode == PERPLEXITY or args.mode == CORPUS_PERP or args.mode == LOG_PROB:
>         x = [line.strip() for line in sys.stdin]
361c246
<             for p in inference.calculate_sentence_perplexity(x, y):
---
>             for p in inference.calculate_sentence_perplexity(x):
363,367c248,260
<         elif args.mode == TRANS_SCORE:
<             for s in inference.calculate_translation_score(x, y):
<                 print(s)
<         else:
<             print(inference.calculate_corpus_perplexity(x, y))
---
>         elif args.mode == CORPUS_PERP:
>             print(inference.calculate_corpus_perplexity(x))
>         elif args.mode == LOG_PROB:
>             for p in inference.calculate_log_prob(x):
>                 print(p)
>     elif args.mode == PMI:
>         with open(args.context) as f:
>             context = f.readlines()
>         with open(args.sentence) as f:
>             sentence = f.readlines()
>         
>         for p in inference.calculate_pmi(context, sentence):
>             print(p)
